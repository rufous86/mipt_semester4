{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10eaee5-8ab7-4621-a404-88118529c86b",
   "metadata": {},
   "source": [
    "# Семинар 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dcafb-64ca-4c99-853b-cf23e4b457e7",
   "metadata": {},
   "source": [
    "Сегодня посмотрим как можно решать задачу text-style transfer, а главное посмотрим как можно подойти к методике валидации качества модели. В качестве задачи мы попробуем обучить модель детоксификации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4cbd3-17bb-4a69-b63c-e387643a50f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f698b93-ff60-4e7c-a483-2b4882dac9f7",
   "metadata": {},
   "source": [
    "## Посмотроим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d48b188-f742-4612-b8e0-1de752f1e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "from typing import Tuple\n",
    "from pathlib import Path \n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12abf16f-9a21-46b9-973d-ad3daf2a6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/input/train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "862f724a-20cf-4f9a-8d85-93ee5228e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep='\\t')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "966dd0cf-9ad4-4d6b-8513-f4636dca9b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment1</th>\n",
       "      <th>neutral_comment2</th>\n",
       "      <th>neutral_comment3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6708</th>\n",
       "      <td>6708</td>\n",
       "      <td>Блин ебаный пиздец хотел с тобой увидится но н...</td>\n",
       "      <td>хотел с тобой увидится но не получилось:( изо ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>979</td>\n",
       "      <td>да надо было е му как широков заебашить прям н...</td>\n",
       "      <td>да надо было ему как широков забить прям на поле</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5739</th>\n",
       "      <td>5739</td>\n",
       "      <td>перезагорал, ебало все облезло( и все(</td>\n",
       "      <td>Перезагорал, лицо всё облезло( и все(</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>ты мразь почему стыдишься русского языка ??</td>\n",
       "      <td>почему вы стыдиться русского языка?</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>1340</td>\n",
       "      <td>шакалы отстаньте от золотого человека уроды ва...</td>\n",
       "      <td>Отстаньте от золотого человека. Вам до него ох...</td>\n",
       "      <td>Отстаньте от золотого человека, вам далеко до ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                      toxic_comment  \\\n",
       "6708   6708  Блин ебаный пиздец хотел с тобой увидится но н...   \n",
       "979     979  да надо было е му как широков заебашить прям н...   \n",
       "5739   5739             перезагорал, ебало все облезло( и все(   \n",
       "147     147        ты мразь почему стыдишься русского языка ??   \n",
       "1340   1340  шакалы отстаньте от золотого человека уроды ва...   \n",
       "\n",
       "                                       neutral_comment1  \\\n",
       "6708  хотел с тобой увидится но не получилось:( изо ...   \n",
       "979    да надо было ему как широков забить прям на поле   \n",
       "5739              Перезагорал, лицо всё облезло( и все(   \n",
       "147                 почему вы стыдиться русского языка?   \n",
       "1340  Отстаньте от золотого человека. Вам до него ох...   \n",
       "\n",
       "                                       neutral_comment2 neutral_comment3  \n",
       "6708                                                                      \n",
       "979                                                                       \n",
       "5739                                                                      \n",
       "147                                                                       \n",
       "1340  Отстаньте от золотого человека, вам далеко до ...                   "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b89309-7e26-47b2-8324-d3c3526273db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6948, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d2453-e32d-4d8f-95af-6528149e283b",
   "metadata": {},
   "source": [
    "## Обучим простую модель T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d2ecdd7-c19f-487e-abea-393ac531085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_toxic = []\n",
    "df_train_neutral = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
    "    \n",
    "    for reference in references:\n",
    "        if len(reference) > 0:\n",
    "            df_train_toxic.append(row['toxic_comment'])\n",
    "            df_train_neutral.append(reference)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f240d21-b671-4122-8991-c10f8e9ab0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'toxic_comment': df_train_toxic,\n",
    "    'neutral_comment': df_train_neutral\n",
    "})\n",
    "\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5ef50-5203-43df-b8ba-317d99134378",
   "metadata": {},
   "source": [
    "Подготовим данные для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "062ed101-dae3-4af8-a97f-1cefaabf0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.x['input_ids'])\n",
    "        item = {key: val[idx] for key, val in self.x.items()}\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n # * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72109744-c4cb-44bc-a812-cff865c8c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "\n",
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        ybatch = self.tokenizer.pad(\n",
    "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = ybatch['input_ids']\n",
    "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33890dd6-5295-43c0-aef1-92c7d100356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55780b10-c643-48e7-9b25-5d9232735d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "            num += len(batch) * loss.item()\n",
    "            den += len(batch)\n",
    "    val_loss = num / den\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78c38bb-e214-495c-961c-574c5e01c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model, train_dataloader, val_dataloader, \n",
    "    max_epochs=30, \n",
    "    max_steps=1_000, \n",
    "    lr=3e-5,\n",
    "    gradient_accumulation_steps=1, \n",
    "    cleanup_step=100,\n",
    "    report_step=300,\n",
    "    window=100,\n",
    "):\n",
    "    cleanup()\n",
    "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "\n",
    "    ewm_loss = 0\n",
    "    step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in trange(max_epochs):\n",
    "        print(step, max_steps)\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        tq = tqdm(train_dataloader)\n",
    "        for i, batch in enumerate(tq):\n",
    "            try:\n",
    "                batch['labels'][batch['labels']==0] = -100\n",
    "                loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print('error on step', i, e)\n",
    "                loss = None\n",
    "                cleanup()\n",
    "                continue\n",
    "            if i and i % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                step += 1\n",
    "                if step >= max_steps:\n",
    "                    break\n",
    "\n",
    "            if i % cleanup_step == 0:\n",
    "                cleanup()\n",
    "\n",
    "            w = 1 / min(i+1, window)\n",
    "            ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "            tq.set_description(f'loss: {ewm_loss:4.4f}')\n",
    "\n",
    "            if (i and i % report_step == 0 or i == len(train_dataloader)-1)  and val_dataloader is not None:\n",
    "                model.eval()\n",
    "                eval_loss = evaluate_model(model, val_dataloader)\n",
    "                model.train()\n",
    "                print(f'epoch {epoch}, step {i}/{step}: train loss: {ewm_loss:4.4f}  val loss: {eval_loss:4.4f}')\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                model.save_pretrained(f't5_base_{dname}_{steps}')\n",
    "        \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a44a317-da83-4979-8874-a64d7d1e9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, y, model_name, test_size=0.1, batch_size=32, **kwargs):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "    train_dataset = PairsDataset(tokenizer(x1), tokenizer(y1))\n",
    "    test_dataset = PairsDataset(tokenizer(x2), tokenizer(y2))\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "    train_loop(model, train_dataloader, val_dataloader, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea3e286-d737-4f22-93bf-768d9f5dd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sberbank-ai/ruT5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b6c15f2-6fa5-4af4-8e17-3240ef39e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "743d8427-baf5-45d5-b705-214f799c5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e464d15-f697-47dd-b0d0-8a5cadc79f54",
   "metadata": {},
   "source": [
    "Попробуем перебирать разное количество шагов обучения для того чтобы понять, когда стоит остановить/продолжить обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c16548c7-343f-4bff-88d0-252db96833f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  train  300 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e2a80103084a8182dd64e8821e75bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d42168d540f43ff88e23453847a1347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n",
      "\n",
      "\n",
      "\n",
      "  train  1000 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26c889b409c42f0803dda2e402c5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34acd97226be43bbb06cb3e0a25adc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 300/300: train loss: 2.6605  val loss: 7.2457\n",
      "epoch 0, step 600/600: train loss: 1.9605  val loss: 7.3611\n",
      "epoch 0, step 623/623: train loss: 1.9363  val loss: 7.1521\n",
      "623 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f41fd7052da491e846c22d0e262aed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 300/923: train loss: 1.6964  val loss: 7.3648\n",
      "1000 1000\n",
      "\n",
      "\n",
      "\n",
      "  train  3000 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f701ff8a2544539a24e16b6ca49c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d111c3ac7fd74175ac09aadc24310111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 300/300: train loss: 2.7260  val loss: 7.4890\n",
      "epoch 0, step 600/600: train loss: 2.0042  val loss: 7.3647\n",
      "epoch 0, step 623/623: train loss: 1.9625  val loss: 7.5061\n",
      "623 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd81d3ff4b7d48a0b2456f849fea413c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 300/923: train loss: 1.7341  val loss: 7.5535\n",
      "epoch 1, step 600/1223: train loss: 1.6099  val loss: 7.3754\n",
      "epoch 1, step 623/1246: train loss: 1.6120  val loss: 7.2570\n",
      "1246 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df64cb55c0044047b9b0aed18d77434d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 300/1546: train loss: 1.5142  val loss: 7.3072\n",
      "epoch 2, step 600/1846: train loss: 1.4540  val loss: 7.2705\n",
      "epoch 2, step 623/1869: train loss: 1.4505  val loss: 7.2934\n",
      "1869 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d84bab52c64891a14319e7affafa78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 300/2169: train loss: 1.3484  val loss: 7.4223\n",
      "epoch 3, step 600/2469: train loss: 1.3705  val loss: 7.2699\n",
      "epoch 3, step 623/2492: train loss: 1.3661  val loss: 7.2257\n",
      "2492 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec748c1a2a463b976bfd78b6d711a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 300/2792: train loss: 1.3017  val loss: 7.3405\n",
      "3000 3000\n",
      "\n",
      "\n",
      "\n",
      "  train  10000 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c31459b71e24e00b43e953b19cb690b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e5647ad9d74352924b26a6641b8551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dname, d \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=====================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoxic_comment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneutral_comment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5_base_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(x, y, model_name, test_size, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[1;32m     11\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_dataloader, val_dataloader, max_epochs, max_steps, lr, gradient_accumulation_steps, cleanup_step, report_step, window)\u001b[0m\n\u001b[1;32m     25\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()})\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror on step\u001b[39m\u001b[38;5;124m'\u001b[39m, i, e)\n",
      "File \u001b[0;32m~/Projects/courses/SF_generation_NLP/.venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/courses/SF_generation_NLP/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for steps in [300, 1000, 3000, 10000]:\n",
    "    for dname, d in datasets.items():\n",
    "        print(f'\\n\\n\\n  {dname}  {steps} \\n=====================\\n\\n')\n",
    "        model = train_model(d['toxic_comment'].tolist(), d['neutral_comment'].tolist(), model_name=model_name, batch_size=16, max_epochs=1000, max_steps=steps)\n",
    "        model.save_pretrained(f't5_base_{dname}_{steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a7969-f4e4-4081-8d17-decd67aabae4",
   "metadata": {},
   "source": [
    "Теперь посмотрим что у нас на inference модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "601d875a-67ba-475a-baf2-6fc7a6c5ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/input/dev.tsv', sep='\\t')\n",
    "toxic_inputs = df['toxic_comment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd071811-72b9-4571-8d94-758779fe3d32",
   "metadata": {},
   "source": [
    "Подгрузим один из обученных нами чекпоинтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2f86f9e-0f96-4af2-adcc-6aed9559320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = 'sberbank-ai/ruT5-base'\n",
    "model_name = './t5_base_train_3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cfc675e-0a41-40df-9bc7-9640ed485cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3dd220b-0660-4145-a95f-2ea27809b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, model, n=None, max_length='auto', temperature=0.0, beams=3):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
    "    result = model.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, \n",
    "        temperature=temperature, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e3ea59a-13d3-4ec4-86b9-5bb56c6bfeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Дмитрий, уже все выложено']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snv/Projects/courses/SF_generation_NLP/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `50.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(['Дмитрий вы ебанулись, уже все выложено'], model, temperature=50.0, beams=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d53432d5-55eb-4e36-8df4-581037a5dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e7048c39da4deeb21aaad9e84a86a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snv/Projects/courses/SF_generation_NLP/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "para_results = []\n",
    "problematic_batch = [] #if something goes wrong you can track such bathces\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(toxic_inputs), batch_size)):\n",
    "    batch = [sentence for sentence in toxic_inputs[i:i + batch_size]]\n",
    "    try:\n",
    "        para_results.extend(paraphrase(batch, model, temperature=0.0))\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        para_results.append(toxic_inputs[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "118663e4-2251-4d01-a06a-1ce5270c5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/output/t5_10000_steps_dev.txt', 'w') as file:\n",
    "    file.writelines([sentence+'\\n' for sentence in para_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45d567-03a3-44fc-8c0b-aa25d6507637",
   "metadata": {},
   "source": [
    "Далее мы с вами посмотрим, а как же можно все таки в автоматизированном режиме оценивать качество получившейся модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156e527-bc4b-42aa-b23b-6395221b49bc",
   "metadata": {},
   "source": [
    "### А как оценивать успешность?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ac2b8-e039-4d5e-853e-a746bb114968",
   "metadata": {},
   "source": [
    "Сохраним в отдельном файле все полезные методы, которые мы будем использовать для автоматической проверки и валидации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84680bc5-b4d0-4e6d-b151-a9129653359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxification_metrics import evaluate_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe4abb44-a69b-4b74-8643-a8f5248cd11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/output/t5_base_10000_dev.txt', 'r') as file:\n",
    "    preds = file.readlines()\n",
    "preds = [sentence.strip() for sentence in preds]\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42baa1-597e-4415-8644-d10ed35fbb66",
   "metadata": {},
   "source": [
    "Воспользуемся открытой моделью-классификатором токсичных текстов, и попробуем ее в качестве первого этапа валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ffb8f-3ef0-438b-a847-090777348f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=None, model=None, tokenizer=None,\n",
    "               model_class=AutoModelForSequenceClassification, use_cuda=True):\n",
    "    if model is None:\n",
    "        if model_name is None:\n",
    "            raise ValueError('Either model or model_name should be provided')\n",
    "        model = model_class.from_pretrained(model_name)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            model.cuda()\n",
    "    if tokenizer is None:\n",
    "        if model_name is None:\n",
    "            raise ValueError('Either tokenizer or model_name should be provided')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f9e2d6-e8b8-4e39-981d-86e70b73f6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d85d84149f94dc283884fa5b1dcf06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f45a81fe64949b0bb78f5d3005b374c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d283ae19e614daab314b23c64d2c28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f6ed764ddf4698bc7d87b8944353f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0d48330ca84d5dab720c751351353d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "style_model, style_tokenizer = load_model('SkolkovoInstitute/russian_toxicity_classifier', use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90f17fa5-65a5-42e8-b5b3-dc1f075ab388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b405ed01d1482dbb7dc63ee430d988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = evaluate_style(\n",
    "    model = style_model,\n",
    "    tokenizer = style_tokenizer,\n",
    "    texts = preds,\n",
    "    target_label=0,  # 1 is toxic, 0 is neutral\n",
    "    batch_size=32, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d729677-6d6f-4e0c-91ab-2dcfce9fc2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style transfer accuracy (STA):  0.753741443157196\n"
     ]
    }
   ],
   "source": [
    "print(f'Style transfer accuracy (STA):  {np.mean(accuracy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25c875-180d-415f-8627-a5af4dac92b6",
   "metadata": {},
   "source": [
    "### метод 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b05e79f0-3b2a-40c3-9228-736455df1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ru_detoxification_metrics import evaluate_cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56991381-1307-4bc0-add7-fc45eb7b8cc2",
   "metadata": {},
   "source": [
    "Энкодер предложений (sentence encoder) – это модель, которая получает на вход текст предложения, а на выходе отдаёт многомерный вектор (например, 768-мерный), примерно описывающий смысл этого предложения. То есть такой, что у предложений, похожих друг на друга по смыслу, векторы похожи друг на друга геометрически. Как мы увидели на прошлом занятии, энкодеры предложений можно использовать для классификации текстов и массы других полезных задач.\n",
    "\n",
    "LaBSE (language-agnostic BERT sentence embeddings) – это модель, предложенная в статье 2020 года от исследователей из Google. По архитектуре это BERT, а обучался он на выборке текстов на 100+ языков в многозадачном режиме. Основная задача – сближать друг с другом эмбеддинги предложений с одинаковым смыслом на разных языках, и с этой задачей модель справляется очень хорошо. Благодаря этой способности можно, например, обучать модель классифицировать английские тексты, а потом применять на русских, или находить в большом корпусе пары предложений на разных языках, являющиеся переводами друг друга."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e851ab-e5b7-4391-82e5-b7e6a92dbc33",
   "metadata": {},
   "source": [
    "Именно поэтому мы воспользуемся данной моделью и посмотрим неа косинусную близость получившихся векторов исходных тектсов и трансформированных моделью Т5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a318054b-0194-4107-bb0e-e7fccd5698b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2200b7daf8074c0fbb129f24af34926c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98848391e2304966a1f7644a3224976b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/516M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6be852d275d443fb457cbd2d4e7515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28cc5c00b57412c909215071c6679d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/521k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb117d388554c6bb32dad9cef22886c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meaning_model, meaning_tokenizer = load_model('cointegrated/LaBSE-en-ru', use_cuda=True, model_class=AutoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2344004-00a7-49c5-a582-dffef13c3190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48450cbe473142c899964ea029340bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1273ad0e594386b35b9cd27864ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity = evaluate_cosine_similarity(\n",
    "    model = meaning_model,\n",
    "    tokenizer = meaning_tokenizer,\n",
    "    original_texts = toxic_inputs,\n",
    "    rewritten_texts = preds,\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bce1fb9-928d-4176-9996-99bd739d03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning preservation (SIM):  0.8054955005645752\n"
     ]
    }
   ],
   "source": [
    "print(f'Meaning preservation (SIM):  {np.mean(similarity)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29af584-76d4-48f1-a856-a4f568326eb8",
   "metadata": {},
   "source": [
    "### метод 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ed6b7ab-710d-469b-9735-d6cb1f23698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ru_detoxification_metrics import evaluate_cola_relative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98cb4-cbfc-41da-b99b-32f13779b275",
   "metadata": {},
   "source": [
    "Модель, которую возьмем для этой задачи создана для оценки естественности коротких текстов на русском языке. Она была обучена отличать тексты, написанные человеком, от их искаженных версий.\n",
    "\n",
    "Источники искажений: случайная замена, удаление, добавление, перетасовка и изменение интонации слов и символов, случайные изменения заглавных букв, обратный перевод, заполнение случайных пробелов с помощью моделей T5 и RoBERTA. Для каждого исходного текста отбирались три поврежденных текста, так что модель равномерно смещена в сторону неестественной метки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2f47182-376a-4367-a484-3c39e55b930b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26746e8c3f04ed29a122432e344d98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681f41361f043c7abceb5b536a96dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53201fef1aff41baa3652df935b8fad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09961db34e92427b8166f47192364f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220a728c7691481d854b821d2323363b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd79050f99d4b1bbbbc94461ee0d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cola_model, cola_tolenizer = load_model('SkolkovoInstitute/rubert-base-corruption-detector', use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "976d4fa1-18f2-48ea-90bd-a3515ba961cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460ad3a80bda4e0db2c811ceaa924179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101c5c15f97f44729c433e6d82221288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fluency = evaluate_cola_relative(\n",
    "    model = cola_model,\n",
    "    tokenizer = cola_tolenizer,\n",
    "    original_texts = toxic_inputs,\n",
    "    rewritten_texts = preds,\n",
    "    target_label=1,\n",
    "    batch_size=32,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671fc7c-984d-48c6-ac30-72eec41cf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Fluency score (FL):  {np.mean(fluency)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32a8eb-130b-4e80-8348-a2583a5ecb4c",
   "metadata": {},
   "source": [
    "### Обобщенная оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a66916-53e5-4d3b-9244-aa2655c0b2ba",
   "metadata": {},
   "source": [
    "Как мы уже разбирали на лекции, зачастую ваша финальная метрика не является каким то одной фиксированной метрикой, или представляет из себя агрегацию/ансамблирование/иерархию сразу нескольких производных метрик.\n",
    "\n",
    "Для этой задачи придумаем похожую аккумулированную оценку и воспользуемся ей как финальной метрикой качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0e497ba-3171-421f-9e13-e0456c4b79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = accuracy * similarity * fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "627d9047-0a39-44b5-8881-89d54619d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint score (J):   0.504615306854248\n"
     ]
    }
   ],
   "source": [
    "print(f'Joint score (J):   {np.mean(joint)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df84355-e0ae-4f61-9d7d-a9f231f774fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c80a2a7-c93c-4459-a958-3a2c9933e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.chrf_score import corpus_chrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2965f23-1dd9-45d2-8f2e-a44a4b0f55f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5790396890355219"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_chrf(neutral_references, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0614fc1-66ee-4c4d-bbf4-411dd43749fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78eea25-6917-45dc-8438-0b372277b68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab1647-b79e-4aeb-9640-ec0ea4b63be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
