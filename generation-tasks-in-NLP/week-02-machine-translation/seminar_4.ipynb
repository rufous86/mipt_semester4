{"cells":[{"cell_type":"markdown","metadata":{"id":"9XvN7xsM7OFz"},"source":["# –°–µ–º–∏–Ω–∞—Ä 4"]},{"cell_type":"markdown","metadata":{"id":"Gq9-Z9DSkT14"},"source":["–°–µ–≥–æ–¥–Ω—è  –º—ã —Å –≤–∞–º–∏ –ø–æ–ø—Ä–æ–±—É–µ–º –¥–æ–æ–±—É—á–∏—Ç—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ NLLB –Ω–∞ –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–π –º–∞—Ä–∏–π—Å–∫–∏–π —è–∑—ã–∫.\n","\n"," –±–∞—à–∫–∏—Ä—Å–∫–∏–π —è–∑—ã–∫ –∫–∞–∫ –∑–≤—É—á–∏—Ç"]},{"cell_type":"markdown","source":["https://huggingface.co/docs/transformers/model_doc/nllb - —Å—Å—ã–ª–∫–∞ –Ω–∞ –º–æ–¥–µ–ª—å\n","\n","- https://huggingface.co/docs/transformers/model_doc/nllb - —Å—Å—ã–ª–∫–∞ –Ω–∞ NLLB\n","- https://arxiv.org/pdf/2310.09917.pdf - —Å—Ä–∞–≤–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã–π—Ö –º–æ–¥–µ–ª–µ–π (mBART, NLLB-200, mPLMs)\n","- https://arxiv.org/pdf/2309.11668.pdf - –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ NLLB\n","- https://arxiv.org/pdf/2309.08565.pdf - –æ–ø–∏—Å–∞–Ω–∏–µ NLLB-200\n","- https://arxiv.org/pdf/2309.03175.pdf - –æ–±–∑–æ—Ä–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –º–æ–¥–µ–ª—è–º\n","- https://arxiv.org/pdf/2306.09830.pdf - —Å—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n","- https://arxiv.org/pdf/2304.04675.pdf - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ GPT-4 –∏ NLLB\n","\n"],"metadata":{"id":"i9VWCA-CRRWo"}},{"cell_type":"markdown","metadata":{"id":"_iBrOtwcjnml"},"source":["# 0. –ü—Ä–µ-—Ä–µ–∫–≤–∏–∑–∏—Ç—ã"]},{"cell_type":"markdown","metadata":{"id":"dc8NcXYHj2Zj"},"source":["`sentencepiece`, –±—ç–∫–µ–Ω–¥ –¥–ª—è –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (–∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏)  \n","`sacremoses`, –ø–∞–∫–µ—Ç, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, —Å –∫–æ—Ç–æ—Ä—ã–º –±—ã–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω—ã –º–æ–¥–µ–ª–∏ NLLB.  \n","`sacrebleu`, –ø–∞–∫–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPjx54id5ko8"},"outputs":[],"source":["import locale\n","def gpe(x=None):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = gpe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xu8BrYo292Nx"},"outputs":[],"source":["!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu  -q"]},{"cell_type":"markdown","metadata":{"id":"OqdSSIVLlCir"},"source":["# 1. Exploring the data\n","\n","In this section, I try to understand what is the training data that I have, and how suitable it is for fine-tuning a NLLB model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUWCMzuwulTn"},"outputs":[],"source":["from datasets import load_dataset\n","\n","data = load_dataset('AigizK/bashkir-russian-parallel-corpora', split='train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7v5UELa6u-3b"},"outputs":[],"source":["data[223]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTyDFaZf984A"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ng7vSnqxvGmZ"},"outputs":[],"source":["trans_df = pd.DataFrame(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6zKA3Fd-M39"},"outputs":[],"source":["print(trans_df.shape)\n","print(trans_df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrJAf8LlfU8S"},"outputs":[],"source":["pd.options.display.max_colwidth = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6ZCvaV1lkkd"},"outputs":[],"source":["trans_df.sample(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKQJ1VPEQ8Kt"},"outputs":[],"source":["trans_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sg7mAjLu7OF_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqQA0thlwGHd"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df_test = trans_df[trans_df.corpus=='1000 sentences'].copy()\n","trans_df = trans_df[trans_df.corpus!='1000 sentences'].sample(100000)\n","\n","df_train_dev, df_dev = train_test_split(trans_df, test_size=0.01)  # 99000 items, 1000 items\n","df_train, df_dev = train_test_split(trans_df, test_size=0.01)  # 99000 items, 1000 items\n","\n","df_train.shape, df_dev.shape, df_test.shape"]},{"cell_type":"markdown","metadata":{"id":"K6qHP-DAA4YD"},"source":["# 2. How well does the data fit into a NLLB tokenizer?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xL261VQtyLl"},"outputs":[],"source":["from transformers import NllbTokenizer\n","from tqdm.auto import tqdm, trange"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05GfWpzKtvcz"},"outputs":[],"source":["tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQywlyv7t9VH"},"outputs":[],"source":["import re\n","\n","def word_tokenize(text):\n","    # a very naive word tokenizer for languages with English-like orthography\n","    return re.findall('(\\w+|[^\\w\\s])', text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzD0htfzuAPu"},"outputs":[],"source":["smpl = df_train.sample(10000, random_state=1)\n","\n","smpl['rus_toks'] = smpl.ru.apply(tokenizer.tokenize)\n","smpl['ba_toks'] = smpl.ba.apply(tokenizer.tokenize)\n","\n","smpl['rus_words'] = smpl.ru.apply(word_tokenize)\n","smpl['ba_words'] = smpl.ba.apply(word_tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrDHIgCwuHeN"},"outputs":[],"source":["smpl.sample(5)[['ba', 'ba_words', 'ba_toks', 'ru', 'rus_words', 'rus_toks']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbgRYDlTuC9z"},"outputs":[],"source":["stats = smpl[['rus_toks', 'ba_toks', 'rus_words', 'ba_words']].applymap(len).describe()\n","stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUJQQzYDuEc5"},"outputs":[],"source":["print(stats.rus_toks['mean'] / stats.rus_words['mean'])\n","print(stats.ba_toks['mean'] / stats.ba_words['mean'])"]},{"cell_type":"markdown","metadata":{"id":"WRASgT937OGC"},"source":["–•–æ—Ä–æ—à–∏–µ –Ω–æ–≤–æ—Å—Ç–∏: –∫–∞–∫ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ, —Ç–∞–∫ –∏ –¥–ª—è –±–∞—à–∫–∏—Ä—Å–∫–æ–≥–æ, —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä NLLB, –∫–∞–∂–µ—Ç—Å—è, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –æ–∫–æ–ª–æ 2 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å–ª–æ–≤–æ (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ, 1.73 –∏ 1.7), —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–æ–Ω–∫–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ NLLB –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–µ–º–ª–µ–º—ã–º –¥–∞–∂–µ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUXEaJlbuqJf"},"outputs":[],"source":["print(tokenizer.unk_token, tokenizer.unk_token_id)"]},{"cell_type":"markdown","metadata":{"id":"27BIJ7HGvKs-"},"source":["–ï—â—ë –æ–¥–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞: –∫–∞–∫ —á–∞—Å—Ç–æ –≤ –≤—ã–≤–æ–¥–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è –±–∞—à–∫–∏—Ä—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω `<unk>`? –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ, –Ω–∞–º –Ω—É–∂–Ω–æ –∫–∞–∫-—Ç–æ —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAEe9lYNu6kv"},"outputs":[],"source":["texts_with_unk = [text for text in tqdm(trans_df.ba) if tokenizer.unk_token_id in tokenizer(text).input_ids]\n","print(len(texts_with_unk))"]},{"cell_type":"markdown","metadata":{"id":"ZdYUmMjn7OGD"},"source":["–¢–µ–∫—Å—Ç—ã –ø–æ–ª—É—á–∏–ª–∏—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥—Ä—è–∑–Ω—ã–º–∏, –ø–æ—ç—Ç–æ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å —Å—Ç–æ–∏—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt5d16TnvSJt"},"outputs":[],"source":["import random\n","s = random.sample(texts_with_unk, 5)\n","s"]},{"cell_type":"markdown","metadata":{"id":"jZhSDbKWvZ0b"},"source":["–ü–æ-–≤–∏–¥–∏–º–æ–º—É, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ —Å 32575 –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –ø—Ä–æ—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—É—é —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—É NLLB.\n","\n","–≠—Ç–æ –ø–æ—Ç–æ–º—É, —á—Ç–æ –º–æ–¥–µ–ª—å NLLB –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö. –ï—Å–ª–∏ –º—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–æ–±–ª–µ–º –±—É–¥–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBXPm8Ckvtf3"},"outputs":[],"source":["# this code is adapted from  the Stopes repo of the NLLB team\n","# https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/monolingual_line_processor.py#L214\n","\n","import re\n","import sys\n","import typing as tp\n","import unicodedata\n","from sacremoses import MosesPunctNormalizer\n","\n","\n","mpn = MosesPunctNormalizer(lang=\"en\")\n","mpn.substitutions = [\n","    (re.compile(r), sub) for r, sub in mpn.substitutions\n","]\n","\n","\n","def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n","    non_printable_map = {\n","        ord(c): replace_by\n","        for c in (chr(i) for i in range(sys.maxunicode + 1))\n","        # same as \\p{C} in perl\n","        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n","        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n","    }\n","\n","    def replace_non_printing_char(line) -> str:\n","        return line.translate(non_printable_map)\n","\n","    return replace_non_printing_char\n","\n","replace_nonprint = get_non_printing_char_replacer(\" \")\n","\n","def preproc(text):\n","    clean = mpn.normalize(text)\n","    clean = replace_nonprint(clean)\n","    # replace ùìïùîØùîûùî´ùî†ùî¢ùî∞ùî†ùîû by Francesca\n","    clean = unicodedata.normalize(\"NFKC\", clean)\n","    return clean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MJp75LAv6Wo"},"outputs":[],"source":["texts_with_unk_normed = [text for text in tqdm(texts_with_unk) if tokenizer.unk_token_id in tokenizer(preproc(text)).input_ids]\n","print(len(texts_with_unk_normed))"]},{"cell_type":"markdown","metadata":{"id":"h5qipLorwBG-"},"source":["–î–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ, –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –º–∞–ª–∞—è —á–∞—Å—Ç—å –∏–∑ –Ω–∏—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.  \n","–≠—Ç–æ –º–æ–∂–µ—Ç —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ–≤–∞—Ç—å –æ —Ç–æ–º, —á—Ç–æ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –±–∞—à–∫–∏—Ä—Å–∫–∏–º —è–∑—ã–∫–æ–º. –û–¥–Ω–∞–∫–æ –º–∞–ª–∞—è —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ –≤—Å–µ –µ—â–µ –æ—Å—Ç–∞–ª–∞—Å—å —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, –ø–æ—ç—Ç–æ–º—É –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä—è –∫—Ä–æ–µ—Ç—Å—è –µ—â–µ –æ–¥–∏–Ω –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–∏"]},{"cell_type":"markdown","metadata":{"id":"vPDFg392tyFB"},"source":["# 3 (optional). Expanding the vocabulary"]},{"cell_type":"markdown","metadata":{"id":"4hUhun80t5u9"},"source":["# 4. Adding a new language tag to the tokenizer and model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhG4XWTP-g3w"},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n","from transformers import NllbTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGh6UDG_-m1K"},"outputs":[],"source":["tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')\n","print(len(tokenizer))\n","print(tokenizer.convert_ids_to_tokens([256202, 256203]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d02fbR_L-nCh"},"outputs":[],"source":["def fix_tokenizer(tokenizer, new_lang='ba_Cyrl'):\n","    \"\"\"\n","    Add a new language token to the tokenizer vocabulary\n","    (this should be done each time after its initialization)\n","    \"\"\"\n","    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n","    tokenizer.lang_code_to_id[new_lang] = old_len-1\n","    tokenizer.id_to_lang_code[old_len-1] = new_lang\n","    # always move \"mask\" to the last position\n","    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n","\n","    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n","    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n","    if new_lang not in tokenizer._additional_special_tokens:\n","        tokenizer._additional_special_tokens.append(new_lang)\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    tokenizer.added_tokens_encoder = {}\n","    tokenizer.added_tokens_decoder = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZ7YPnHQ-pDT"},"outputs":[],"source":["fix_tokenizer(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppwnJUrj-rLu"},"outputs":[],"source":["print(tokenizer.convert_ids_to_tokens([256202, 256203, 256204])) # ['zul_Latn', 'ba_Cyrl', '<mask>']\n","print(tokenizer.convert_tokens_to_ids(['zul_Latn', 'ba_Cyrl', '<mask>'])) # [256202, 256203, 256204]\n","# this is consistent now, wow!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktO8outV-xws"},"outputs":[],"source":["added_token_id = tokenizer.convert_tokens_to_ids('ba_Cyrl')\n","similar_lang_id = tokenizer.convert_tokens_to_ids('kir_Cyrl')\n","print(added_token_id, similar_lang_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLlwR3_R-tDL"},"outputs":[],"source":["model = AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lV-fIcWZ-3WJ"},"outputs":[],"source":["# moving the embedding for \"mask\" to its new position\n","model.model.shared.weight.data[added_token_id+1] = model.model.shared.weight.data[added_token_id]\n","# initializing new language token with a token of a similar language\n","model.model.shared.weight.data[added_token_id] = model.model.shared.weight.data[similar_lang_id]"]},{"cell_type":"markdown","metadata":{"id":"5ssJCguZ-3oH"},"source":["# 5. Preparing the training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjuuYbpG-7nS"},"outputs":[],"source":["import gc\n","import random\n","import numpy as np\n","import torch\n","from tqdm.auto import tqdm, trange\n","from transformers.optimization import Adafactor\n","from transformers import get_constant_schedule_with_warmup\n","\n","def cleanup():\n","    \"\"\"Try to free GPU memory\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","cleanup()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olSkAk2p-9IE"},"outputs":[],"source":["model.cuda();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScoroAeY-_-J"},"outputs":[],"source":["optimizer = Adafactor(\n","    [p for p in model.parameters() if p.requires_grad],\n","    scale_parameter=False,\n","    relative_step=False,\n","    lr=1e-4,\n","    clip_threshold=1.0,\n","    weight_decay=1e-3,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9cxb-64_Bco"},"outputs":[],"source":["batch_size = 16  # 32 already doesn't fit well to 15GB of GPU memory\n","max_length = 128\n","warmup_steps = 1_000\n","training_steps = 57000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tbPSr7w_Hnp"},"outputs":[],"source":["losses = []\n","scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H15rBohL_MaC"},"outputs":[],"source":["LANGS = [('ru', 'rus_Cyrl'), ('ba', 'ba_Cyrl')]\n","\n","def get_batch_pairs(batch_size, data=df_train):\n","    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n","    xx, yy = [], []\n","    for _ in range(batch_size):\n","        item = data.iloc[random.randint(0, len(data)-1)]\n","        xx.append(preproc(item[l1]))\n","        yy.append(preproc(item[l2]))\n","    return xx, yy, long1, long2\n","\n","print(get_batch_pairs(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ACqa2if_N5U"},"outputs":[],"source":["MODEL_SAVE_PATH = './models/nllb-rus-ba-v1'"]},{"cell_type":"markdown","metadata":{"id":"V1BV9mcZwmLd"},"source":["# 6. The training loop"]},{"cell_type":"markdown","metadata":{"id":"_itEkiRS7OGX"},"source":["–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –Ω–∞ 100k –ø—Ä–∏–º–µ—Ä–∞—Ö –∑–∞–Ω–∏–º–∞–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 5 —á–∞—Å–æ–≤, —Ç–∞–∫ —á—Ç–æ –∑–∞–ø–∞—Å–∏—Ç–µ—Å—å —Ç–µ—Ä–ø–µ–Ω–∏–µ–º"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ahPBT-vt_c91"},"outputs":[],"source":["model.train()\n","x, y, loss = None, None, None\n","cleanup()\n","\n","tq = trange(len(losses), training_steps)\n","for i in tq:\n","    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n","    try:\n","        tokenizer.src_lang = lang1\n","        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n","        tokenizer.src_lang = lang2\n","        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n","        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n","\n","        loss = model(**x, labels=y.input_ids).loss\n","        loss.backward()\n","        losses.append(loss.item())\n","\n","        optimizer.step()\n","        optimizer.zero_grad(set_to_none=True)\n","        scheduler.step()\n","\n","    except RuntimeError as e:\n","        optimizer.zero_grad(set_to_none=True)\n","        x, y, loss = None, None, None\n","        cleanup()\n","        print('error', max(len(s) for s in xx + yy), e)\n","        continue\n","\n","    if i % 1000 == 0:\n","        print(i, np.mean(losses[-1000:]))\n","\n","    if i % 1000 == 0 and i > 0:\n","        model.save_pretrained(MODEL_SAVE_PATH)\n","        tokenizer.save_pretrained(MODEL_SAVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXXT9pcd_9Au"},"outputs":[],"source":["pd.Series(losses).ewm(100).mean().plot();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MGVf4Vc_fS4"},"outputs":[],"source":["def translate(text, src_lang='rus_Cyrl', tgt_lang='eng_Latn', a=16, b=1.5, max_input_length=1024, **kwargs):\n","    tokenizer.src_lang = src_lang\n","    tokenizer.tgt_lang = tgt_lang\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n","    result = model.generate(\n","        **inputs.to(model.device),\n","        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n","        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n","        **kwargs\n","    )\n","    #print(inputs.input_ids.shape[1], result.shape[1])\n","    return tokenizer.batch_decode(result, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c69XqtpbAgjN"},"outputs":[],"source":["xx, yy, lang1, lang2 = get_batch_pairs(1, data=df_dev)\n","print(xx)\n","print(yy)\n","model.eval()\n","print(translate(xx[0], lang1, lang2, no_repeat_ngram_size=3, num_beams=5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCZR50GxAiPJ"},"outputs":[],"source":["!ls -alsh $MODEL_SAVE_PATH"]},{"cell_type":"markdown","metadata":{"id":"0qubmjZNAxJB"},"source":["# 6. Using the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKGZ8zuN2mV6"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import NllbTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n","from tqdm.auto import tqdm, trange"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AXtm-Qf2wCR"},"outputs":[],"source":["# this code is adapted from  the Stopes repo of the NLLB team\n","# https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/monolingual_line_processor.py#L214\n","\n","import re\n","import sys\n","import typing as tp\n","import unicodedata\n","from sacremoses import MosesPunctNormalizer\n","\n","\n","mpn = MosesPunctNormalizer(lang=\"en\")\n","mpn.substitutions = [\n","    (re.compile(r), sub) for r, sub in mpn.substitutions\n","]\n","\n","\n","def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n","    non_printable_map = {\n","        ord(c): replace_by\n","        for c in (chr(i) for i in range(sys.maxunicode + 1))\n","        # same as \\p{C} in perl\n","        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n","        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n","    }\n","\n","    def replace_non_printing_char(line) -> str:\n","        return line.translate(non_printable_map)\n","\n","    return replace_non_printing_char\n","\n","replace_nonprint = get_non_printing_char_replacer(\" \")\n","\n","def preproc(text):\n","    clean = mpn.normalize(text)\n","    clean = replace_nonprint(clean)\n","    # replace ùìïùîØùîûùî´ùî†ùî¢ùî∞ùî†ùîû by Francesca\n","    clean = unicodedata.normalize(\"NFKC\", clean)\n","    return clean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wwb6ck8P25ZQ"},"outputs":[],"source":["def fix_tokenizer(tokenizer, new_lang='ba_Cyrl'):\n","    \"\"\" Add a new language token to the tokenizer vocabulary (this should be done each time after its initialization) \"\"\"\n","    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n","    tokenizer.lang_code_to_id[new_lang] = old_len-1\n","    tokenizer.id_to_lang_code[old_len-1] = new_lang\n","    # always move \"mask\" to the last position\n","    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n","\n","    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n","    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n","    if new_lang not in tokenizer._additional_special_tokens:\n","        tokenizer._additional_special_tokens.append(new_lang)\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    tokenizer.added_tokens_encoder = {}\n","    tokenizer.added_tokens_decoder = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uY7nUGsX3NOM"},"outputs":[],"source":["model_load_name = './models/nllb-rus-ba-v1'\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()\n","tokenizer = NllbTokenizer.from_pretrained(model_load_name)\n","fix_tokenizer(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIsPI6YT3UG0"},"outputs":[],"source":["def translate(text, src_lang='rus_Cyrl', tgt_lang='eng_Latn', a=32, b=3, max_input_length=1024, num_beams=4, **kwargs):\n","    tokenizer.src_lang = src_lang\n","    tokenizer.tgt_lang = tgt_lang\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n","    result = model.generate(\n","        **inputs.to(model.device),\n","        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n","        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n","        num_beams=num_beams,\n","        **kwargs\n","    )\n","    return tokenizer.batch_decode(result, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoWvizFCRngQ"},"outputs":[],"source":["def batched_translate(texts, batch_size=16, **kwargs):\n","    \"\"\"Translate texts in batches of similar length\"\"\"\n","    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n","    results = []\n","    for i in trange(0, len(texts2), batch_size):\n","        results.extend(translate(texts2[i: i+batch_size], **kwargs))\n","    return [p for i, p in sorted(zip(idxs, results))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-wPl4fTRlv2"},"outputs":[],"source":["rus_translated = batched_translate(df_dev.ba, src_lang='ba_Cyrl', tgt_lang='rus_Cyrl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3S475uG3fBh"},"outputs":[],"source":["df_dev['rus_translated'] = [translate(t, 'ba_Cyrl', 'rus_Cyrl')[0] for t in tqdm(df_dev.ba)]\n","df_dev['ba_translated'] = [translate(t, 'rus_Cyrl', 'ba_Cyrl')[0] for t in tqdm(df_dev.ru)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMRSCWW732ya"},"outputs":[],"source":["import sacrebleu\n","bleu_calc = sacrebleu.BLEU()\n","chrf_calc = sacrebleu.CHRF(word_order=2)  # this metric is called ChrF++"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSwL7o6DMy8M"},"outputs":[],"source":["xx, yy = ['—Ç–µ—á—ë—Ç —Ö–æ–ª–æ–¥'], ['–Ω–µ—Å—ë—Ç —Ö–æ–ª–æ–¥–æ–º']\n","print(bleu_calc.corpus_score(xx, [yy]))\n","print(chrf_calc.corpus_score(xx, [yy]))\n","print(chrf_calc.corpus_score(yy, [xx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NKKUyXZ4oXr"},"outputs":[],"source":["print(bleu_calc.corpus_score(df_dev['rus_translated'].tolist(), [df_dev['ru'].tolist()]))\n","print(chrf_calc.corpus_score(df_dev['rus_translated'].tolist(), [df_dev['ru'].tolist()]))\n","print(bleu_calc.corpus_score(df_dev['ba_translated'].tolist(), [df_dev['ba'].tolist()]))\n","print(chrf_calc.corpus_score(df_dev['ba_translated'].tolist(), [df_dev['ba'].tolist()]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svplVgTB5_Xq"},"outputs":[],"source":["pd.options.display.max_colwidth = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3bMbXUv5TsV"},"outputs":[],"source":["df_dev.sample(10, random_state=5)[['ba', 'ru', 'ba_translated', 'rus_translated']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCEO4dSgJYKE"},"outputs":[],"source":["print((df_dev.ru == df_dev.rus_translated).mean())\n","print((df_dev.ba == df_dev.ba_translated).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NEfm2fmJm1S"},"outputs":[],"source":["!pip install editdistance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxxFrdC7JrS6"},"outputs":[],"source":["import editdistance\n","\n","def ed_similarity(text1, text2):\n","    return max(0, 1 - editdistance.eval(text1, text2) / min(len(text1), len(text2)))\n","\n","print(ed_similarity('–∫–æ—Ç', '—Å–æ–±–∞–∫–∞'))\n","print(ed_similarity('–∫–æ—Ç', '–∫–∏—Ç'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjkvAyN9JyXg"},"outputs":[],"source":["pd.Series([ed_similarity(row.ru, row.rus_translated) for row in df_dev.itertuples()]).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXm7oczQKknv"},"outputs":[],"source":["pd.Series([ed_similarity(row.ba, row.ba_translated) for row in df_dev.itertuples()]).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OL-s6bK6UIE"},"outputs":[],"source":["df_dev.index.name = \"row_id\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjmFAfB355Ss"},"outputs":[],"source":["df_dev.to_csv(model_load_name + \"/dev_set_translated.tsv\", sep=\"\\t\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Whjo__Sb0CeA"},"outputs":[],"source":["def translate(\n","    text,\n","    model,\n","    tokenizer,\n","    src_lang='rus_Cyrl',\n","    tgt_lang='ba_Cyrl',\n","    max_length='auto',\n","    num_beams=4,\n","    no_repeat_ngram_size=4,\n","    n_out=None,\n","    **kwargs\n","):\n","    tokenizer.src_lang = src_lang\n","    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","    if max_length == 'auto':\n","        max_length = int(32 + 2.0 * encoded.input_ids.shape[1])\n","    model.eval()\n","    generated_tokens = model.generate(\n","        **encoded.to(model.device),\n","        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],\n","        max_length=max_length,\n","        num_beams=num_beams,\n","        no_repeat_ngram_size=no_repeat_ngram_size,\n","        num_return_sequences=n_out or 1,\n","        **kwargs\n","    )\n","    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","    if isinstance(text, str) and n_out is None:\n","        return out[0]\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79sB2Zd6RA49"},"outputs":[],"source":["translate(\"–∫—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞\", model=model, tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"66sAzGnX6clC"},"source":["–î–æ–±–∏—Ç—å—Å—è –±–æ–ª–µ–µ —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–º–∏–º–æ –æ—á–∏—Å—Ç–∫–∏ –≤–∞–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ –∑–∞ —Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –¥–ª—è –≤–∞—à–µ–≥–æ —Ü–µ–ª–µ–≤–æ–≥–æ —è–∑—ã–∫–∞ - —ç—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –æ–±—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –Ω—É–ª—è –∏ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –≤–∞—à –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∞ –ø–æ–∑–∂–µ –∏ –º–æ–¥–µ–ª—å"]},{"cell_type":"markdown","metadata":{"id":"RXLceZcp7OGf"},"source":["–ü–æ–º–∏–º–æ –≤—Å–µ–≥–æ –ø—Ä–æ—á–µ–≥–æ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"]},{"cell_type":"markdown","metadata":{"id":"T9xxiz1H7OGf"},"source":["–î–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å–∞–º—ã–º –¥–µ—à–µ–≤—ã–º –º–µ—Ç–æ–¥–æ–º —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ beam_size –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ beam-search. –û–¥–Ω–∞–∫–æ —Å—Ç–æ–∏—Ç –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ —ç—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –∑–∞–º–µ–¥–ª–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏\n","\n","–¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∑–∞ —Å—á–µ—Ç –±–æ–ª–µ–µ –¥–æ–ª–≥–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –≤–∞—à–µ–º –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω–æ–º —è–∑—ã–∫–µ, –æ–¥–Ω–∞–∫–æ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –±—É–¥–µ—Ç –Ω–µ–º–Ω–æ–≥–æ –ø—Ä–æ—Å–∞–∂–∏–≤–∞—Ç—å—Å—è"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2DWArceRsM-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}